# Import Gym Dependencies
import gym
from gym import Env
from gym.spaces import Discrete, Box, Dict, Tuple, MultiBinary, MultiDiscrete

# Importing Helpers Libraries
import numpy as np
import random
import os

# Importing Stable-Baselines3 Stuff
from stable_baselines3 import ppo
from stable_baselines3.common.vec_env import dummy_vec_env
from stable_baselines3.common.evaluation import evaluate_policy

# Observing what the spaces look like
print(Discrete(3).sample())
print(Box(low=0, high=1, shape=(4,4)).sample())
print(Tuple((Discrete(1), Discrete(10))).sample())
print(Dict({"Position": Discrete(2), "Velocity": Discrete(3)}).sample())
print(MultiBinary([3,3]).sample())
print(MultiDiscrete([10,2,3,4]).sample())

# Simulated Environment: 
"""
To escape a 5x5 hex maze, agent starts on one hex, and must avoid obstacles to find the most optimized path to the end
action_space: can move left, right, up, down
"""
class mazeEscape(Env):
    def __init__(self) -> None:
        self.action_space = Discrete(4)
        self.observation_space = 
        super().__init__()

    def step(self, action):
        """
        Take an action from the action space and have the RL model apply it to the board
        """
        action
        pass

    def render(self):
        """
        Render and show the current state of the game, where the agent's at and what the current map looks like
        """

        pass

    def reset(self):
        """
        Resets the agent to the start and resets the map
        """
        pass